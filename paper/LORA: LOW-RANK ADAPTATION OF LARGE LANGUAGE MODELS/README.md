# 1. Introduction

## 1.1 downstream & upstream

+ downstream
  + ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì‘ìš© ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.
+ upstream
  + ëª¨ë¸ í•™ìŠµ ì´ì „ì˜ ë‹¨ê³„ë¡œ, ë°ì´í„°ì˜ ì¤€ë¹„ë‚˜ ì‚¬ì „ í•™ìŠµ ìì²´ì— í•´ë‹¹í•©ë‹ˆë‹¤.

## 1.2 LoRA

+ Problem
  + ë¯¸ì„¸ ì¡°ì •ì˜ ë‹¨ì ì€ ìƒˆë¡œìš´ ëª¨ë¸ì´ ì›ë˜ ëª¨ë¸ê³¼ ë™ì¼í•œ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ê²Œ ë˜ì–´, ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œëŠ” ì €ì¥ê³¼ ë°°í¬ê°€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë¼ëŠ” ì ì…ë‹ˆë‹¤. íŠ¹íˆ GPT-3ì™€ ê°™ì€ 1750ì–µ ê°œì˜ íŠ¸ë ˆì´ë„ˆë¸” íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ëª¨ë¸ì—ì„œëŠ” í° ë¬¸ì œê°€ ë©ë‹ˆë‹¤.

+ Idea
  + í•™ìŠµëœ ê³¼ëŒ€ íŒŒë¼ë¯¸í„° ëª¨ë¸ì—ëŠ” ì‚¬ì‹¤ ë‚´ì¬ëœ ì €ì°¨ì›(rank)ì´ ì¡´ì¬í•œë‹¤ëŠ” ê¸°ì¡´ ì—°êµ¬ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. LoRAëŠ” ì´ ì €ì°¨ì›ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì˜ ì ì‘ ì‹œ ë³€ê²½ë˜ëŠ” ë°€ì§‘ ê³„ì¸µì„ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.

+ Method
  + ë°€ì§‘ ê³„ì¸µì˜ ë³€í™”(íŒŒë¼ë¯¸í„° ë³€ê²½)ë¥¼ ì €ì°¨ì› í–‰ë ¬ë¡œ ë¶„í•´í•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜(Pre-trained Weights)ëŠ” ê³ ì •í•˜ê³ , ìƒˆë¡œìš´ ì‘ì—…ì—ì„œ í•„ìš”í•œ ì ì‘ì„ ìœ„í•´ ì‘ì€ íŒŒë¼ë¯¸í„°(í–‰ë ¬ Aì™€ B)ë§Œì„ í•™ìŠµí•©ë‹ˆë‹¤.

## 1.3 Terminologies

+ $d_{model}$
  
  + Transformer layerì˜ ì…ë ¥ ë° ì¶œë ¥ ì°¨ì›ì˜ í¬ê¸°

+ $W_{q}$, $W_{k}$, $W_{v}$, $W_{o}$
  
  + self-attention ëª¨ë“ˆì—ì„œ ì¿¼ë¦¬(query), í‚¤(key), ê°’(value), ì¶œë ¥(output) í”„ë¡œì ì…˜ í–‰ë ¬ì„ ê°ê° ì§€ì¹­

+ $r$
  
  + LoRA moduleì˜ rankë¥¼ ì§€ì¹­

# 2. Problem Statement

ê¸°ì¡´ Fine Tuning ê³¼ì •
> <img width="194" alt="image" src="https://github.com/user-attachments/assets/5345f498-f792-434d-8aaa-3e0940084aff">

LoRA
> <img width="233" alt="image" src="https://github.com/user-attachments/assets/9e4a4a0e-d386-4cb2-a512-9ef0b37462dc">

# 3. Aren't Existing Solutions Good Enough

Transformerì— ì¶”ê°€ë˜ëŠ” Adapter

<img width="486" alt="image" src="https://github.com/user-attachments/assets/f6567de9-06da-487e-a2bb-7f3c251d08f4">  

  

Adapterì˜ ì¥ì 

+ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±
  + ì–´ëŒ‘í„°ëŠ” ì›ë˜ ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„°ì— ë¹„í•´ ë§¤ìš° ì ì€ ì–‘ì˜ íŒŒë¼ë¯¸í„°ë§Œì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›ë˜ ëª¨ë¸ì˜ 1% ë¯¸ë§Œì˜ íŒŒë¼ë¯¸í„°ë§Œì„ ì¶”ê°€ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ ì‘ì—…ì— ëŒ€í•´ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    
+ ëª¨ë¸ ê³µìœ ì˜ ìš©ì´ì„±
  + ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ë™ì¼í•œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì—¬ëŸ¬ ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ì‘ì—…ì— í•„ìš”í•œ ì–´ëŒ‘í„°ë§Œ êµì²´í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì—, ì €ì¥ ë° ë°°í¬ê°€ ìš©ì´í•©ë‹ˆë‹¤.
 
Adapterì˜ ë‹¨ì 

+ ì¶”ë¡  ì§€ì—°(Inference Latency)
  + ì–´ëŒ‘í„° ë ˆì´ì–´ëŠ” ëª¨ë¸ì— ì¶”ê°€ì ì¸ ì—°ì‚°ì„ ìš”êµ¬í•©ë‹ˆë‹¤. íŠ¹íˆ, ëŒ€ê·œëª¨ ì‹ ê²½ë§ì—ì„œ ì–´ëŒ‘í„° ë ˆì´ì–´ëŠ” ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ëª¨ë¸ì˜ ì¶”ë¡  ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

+ í”„ë¡¬í”„íŠ¸ íŠœë‹ì˜ ì–´ë ¤ì›€
  + ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ëŒ€ì‹  í”„ë¡¬í”„íŠ¸ë¥¼ ì§ì ‘ ìµœì í™”í•˜ëŠ” ì ‘ê·¼ë²•(í”„ë¦¬í”½ìŠ¤ íŠœë‹ ë“±)ì€ ë˜ ë‹¤ë¥¸ ë¬¸ì œë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ íŠœë‹ì€ ìµœì í™”ê°€ ì–´ë µê³ , í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ì„±ëŠ¥ì´ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.

# 4. Our Method

## 4.1 LoRAì˜ ê¸°ë³¸ Idea

LoRAëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ì €ì°¨ì› í–‰ë ¬(ì €ì°¨ì› ë­í¬)ì„ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¬íŒŒë¼ë¯¸í„°í™”í•©ë‹ˆë‹¤. 
ì´ë¥¼ í†µí•´ ì›ë˜ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³ ì •í•œ ì±„, ë” ì‘ì€ í¬ê¸°ì˜ í–‰ë ¬ë§Œ í•™ìŠµí•˜ê²Œ ë˜ì–´ ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë¹„ìš©ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì €ì°¨ì› ë­í¬
> LoRAëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ì €ì°¨ì›(rank) ê³µê°„ì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ $W_0+BA$ì˜ í˜•íƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $ğ´$ì™€ $B$ëŠ” ì‘ì€ ì°¨ì›ì˜ í–‰ë ¬ì…ë‹ˆë‹¤.
> ì´ ë°©ë²•ì„ í†µí•´ ì „ì²´ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œë„, ì‹¤ì œë¡œëŠ” í›¨ì”¬ ì ì€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.


# 5. Empirical Experiments

Baseline

+ FT: Fine-Tuning
+ FT_Top2: ë§ˆì§€ë§‰ ë‘ ë ˆì´ì–´ë§Œ íŠœë‹
+ BitFit
+ Adap_H: ì˜¤ë¦¬ì§€ë„ adapter tuning
+ Adap_L: MLP ëª¨ë“ˆ ë’¤ì™€ LayerNorm ë’¤ì—ë§Œ adapter layer ì ìš©
+ Adap_P: AdapterFusion (Adap_Lê³¼ ìœ ì‚¬)
+ Adap_D: AdapterDrop (ëª‡ëª‡ adapter layerë¥¼ drop)

<img width="520" alt="image" src="https://github.com/user-attachments/assets/15928c1e-260e-4eef-84a3-98d22eae896e">

<img width="400" alt="image" src="https://github.com/user-attachments/assets/6d6a65c6-1db4-428f-8930-e4045a3b0453">


